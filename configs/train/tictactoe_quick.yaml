# TicTacToe Quick Training Configuration
# =====================================
# Fast training for testing the pipeline (completes in ~5-10 minutes)
# Use this to verify your setup works before running full training

game: tictactoe

algorithm:
  name: alphazero

  # Hardware device - update based on your system
  device: mps  # Options: 'cpu', 'cuda' (NVIDIA GPU), 'mps' (Apple Silicon)

  # Model architecture
  model:
    class: TicTacToeMLPNet
    hidden: 64

  # Training loop
  iterations: 100                    # Reduced for quick testing
  games_per_iteration: 8             # Self-play games per iteration
  batch_size: 64                     # Training batch size
  train_steps_per_iteration: 50      # Gradient steps per iteration
  lr: 0.003                          # Learning rate
  weight_decay: 0.0001               # L2 regularization
  value_loss_coef: 1.0               # Weight for value loss

  # Self-play behavior
  temp_moves: 4                      # Number of moves with temperature exploration
  tau: 1.0                           # Temperature for exploration
  deterministic_after_temp: true     # Use argmax after temp_moves
  add_dirichlet_noise: true          # Add root noise for exploration

  # MCTS search
  num_sims: 50                       # MCTS simulations per move
  c_puct: 1.25                       # PUCT exploration constant
  dirichlet_alpha: 0.6               # Dirichlet noise concentration
  dirichlet_eps: 0.2                 # Dirichlet noise weight
  illegal_action_penalty: 1000000000.0

  # Experience buffer
  buffer_capacity: 40000             # Max samples in replay buffer
  clear_mcts_each_game: true         # Clear search tree between games

# Notes:
# ------
# - This config is for QUICK TESTING, not production training
# - Agent will be weak but training completes fast
# - For competitive agent, use tictactoe_full.yaml instead
# - Trained model will be saved to saved_agents/
